# Ollama Configuration

# Ollama server URL (default: http://localhost:11434)
# For WSL accessing Windows Ollama, use the Windows host IP:
#   OLLAMA_HOST=http://172.x.x.x:11434
OLLAMA_HOST=http://localhost:11434

# Model to use for experiments (default: llama3.2:3b)
# Other options: llama3.2:1b, llama3.1:8b, mistral:7b, etc.
MODEL_NAME=llama3.2:3b
